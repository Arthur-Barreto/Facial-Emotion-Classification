{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# from spektral.layers import GraphConv\n",
    "# GRaphConv is deprecated, use GCNConv or GCSConv instead\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.layers import GCSConv  # as GraphConv\n",
    "from spektral.layers import GINConv # as GraphConv\n",
    "from spektral.layers import GCNConv  # as GraphConv\n",
    "\n",
    "from spektral.utils.convolution import gcn_filter  # For GCNConv\n",
    "from spektral.utils.convolution import normalized_adjacency  # For GCSConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "l2_reg = 5e-4  # Regularization rate for l2\n",
    "learning_rate = 1e-3  # Learning rate for SGD\n",
    "batch_size = 32  # Batch size\n",
    "epochs = 5  # Number of training epochs\n",
    "es_patience = 200  # Patience fot early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\Documents\\Rodrigo\\Insper\\SextoSemestre\\Facial-Emotion-Classification-Graph_Fork\\angry_adj\n",
      "8024\n"
     ]
    }
   ],
   "source": [
    "# Load one adjacency matrix and its features\n",
    "# teste\n",
    "# load 10 npz files from the folder angry_adj\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "actual_path = pathlib.Path().absolute()\n",
    "\n",
    "# subindo um nível\n",
    "\n",
    "path = actual_path.parent\n",
    "\n",
    "\n",
    "# load the npz file\n",
    "# the npz file is a sparse matrix\n",
    "\n",
    "adj_angry_path = path / 'angry_adj'\n",
    "print(adj_angry_path)\n",
    "for file in os.listdir(adj_angry_path)[:1]:\n",
    "    file_path = adj_angry_path / file\n",
    "    sparse_matrix = scipy.sparse.load_npz(file_path)\n",
    "    sparse_matrix = sparse_matrix.todense()\n",
    "\n",
    "# as all the matrices are equal, we can use the first one is equal to all the others\n",
    "\n",
    "angry_path = path / 'surprised_dist'\n",
    "disgusted_path = path / 'disgusted_dist'\n",
    "happy_path = path / 'happy_dist'\n",
    "neutral_path = path / 'neutral_dist'\n",
    "sad_path = path / 'sad_dist'\n",
    "surprised_path = path / 'surprised_dist'\n",
    "\n",
    "path_list = [angry_path, disgusted_path, happy_path, neutral_path, sad_path, surprised_path]\n",
    "\n",
    "# load the json files of the distances (features)\n",
    "\n",
    "import json\n",
    "\n",
    "# load the json files\n",
    "dists_surprised = []\n",
    "dists_disgusted = []\n",
    "dists_happy = []\n",
    "dists_neutral = []\n",
    "dists_sad = []\n",
    "dists_angry = []\n",
    "\n",
    "dists_list = [dists_angry, dists_disgusted, dists_happy, dists_neutral, dists_sad, dists_surprised]\n",
    "\n",
    "for path in path_list:\n",
    "    for file in os.listdir(path):\n",
    "        file_path = path / file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dists_list[path_list.index(path)].append(data)\n",
    "\n",
    "print(len(dists_angry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 468)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n"
     ]
    }
   ],
   "source": [
    "# dimension of the target\n",
    "n_out = 6\n",
    "\n",
    "N = (sparse_matrix.shape[0])  # Number of nodes in the graph\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the features\n",
    "F = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "y = np.arange(n_out)  # Target labels\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 468)\n",
      "[[ 0.         89.63977946  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [41.13040241  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# create a mtrix that is the adjancency matrix multiplied by the features\n",
    "# the features are the distances between the nodes\n",
    "# each line of the matrix is a node and the columns are the distances between the node and the other nodes\n",
    "# para cada ponto = 1 na matriz de adjacencia, multiplica pela distancia entre os pontos\n",
    "# o resultado é a matriz de features\n",
    "\n",
    "def create_features_matrix(adj_matrix, dists_list):\n",
    "    features_matrix = np.zeros((adj_matrix.shape[0], adj_matrix.shape[0]))\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                #print(dists_list[i])\n",
    "                features_matrix[i, j] = dists_list[i]\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "features_matrix = create_features_matrix(sparse_matrix, dists_list[0][0])\n",
    "print(features_matrix.shape)\n",
    "print(features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each distance matrix, create a features matrix\n",
    "features_matrix_list = []\n",
    "for dists in dists_list:\n",
    "    for image_dists in dists:\n",
    "        features_matrix_list.append(create_features_matrix(sparse_matrix, image_dists))\n",
    "\n",
    "print(len(features_matrix_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The GINConv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This model does nor need normalization\n",
    "\n",
    "\n",
    "# Model using GCSConv\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "A_in = Input(tensor=sp_matrix_to_sp_tensor(sparse_matrix))\n",
    "\n",
    "graph_conv_1 = GINConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([X_in, A_in])\n",
    "graph_conv_2 = GINConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([graph_conv_1, A_in])\n",
    "flatten = Flatten()(graph_conv_2)\n",
    "fc = Dense(512, activation=\"relu\")(flatten)\n",
    "output = Dense(n_out, activation=\"softmax\")(fc)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the GCSConv Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://graphneural.network/layers/convolution/#gcsconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\envs\\mirror\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 468, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(468, 468)]                 0         []                            \n",
      "                                                                                                  \n",
      " gcs_conv (GCSConv)          (None, 468, 32)              96        ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " gcs_conv_1 (GCSConv)        (None, 468, 32)              2080      ['gcs_conv[0][0]',            \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 14976)                0         ['gcs_conv_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 512)                  7668224   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 6)                    3078      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7673478 (29.27 MB)\n",
      "Trainable params: 7673478 (29.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "fltr = normalized_adjacency(sparse_matrix)\n",
    "\n",
    "# Model using GCSConv\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "graph_conv_1 = GCSConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([X_in, A_in])\n",
    "graph_conv_2 = GCSConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([graph_conv_1, A_in])\n",
    "flatten = Flatten()(graph_conv_2)\n",
    "fc = Dense(512, activation=\"relu\")(flatten)\n",
    "output = Dense(n_out, activation=\"softmax\")(fc)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GSCConv model\n",
    "model.fit(\n",
    "    [nodes, fltr],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the GCNConv Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://graphneural.network/layers/convolution/#gcnconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "fltr = gcn_filter(adj)\n",
    "# Model using GCNConv\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "graph_conv_1 = GCNConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([X_in, A_in])\n",
    "graph_conv_2 = GCNConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([graph_conv_1, A_in])\n",
    "# removed the flatten layer because it is not necessary for GCNConv\n",
    "fc = Dense(512, activation=\"relu\")(graph_conv_2)\n",
    "output = Dense(n_out, activation=\"softmax\")(fc)\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GCNConv model\n",
    "\n",
    "model.fit(\n",
    "    [nodes, fltr],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirror",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
