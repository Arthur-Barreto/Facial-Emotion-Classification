{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# from spektral.layers import GraphConv\n",
    "# GRaphConv is deprecated, use GCNConv or GCSConv instead\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.layers import GCSConv  # as GraphConv\n",
    "from spektral.layers import GINConv # as GraphConv\n",
    "from spektral.layers import GCNConv  # as GraphConv\n",
    "\n",
    "from spektral.utils.convolution import gcn_filter  # For GCNConv\n",
    "from spektral.utils.convolution import normalized_adjacency  # For GCSConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "l2_reg = 5e-4  # Regularization rate for l2\n",
    "learning_rate = 1e-3  # Learning rate for SGD\n",
    "batch_size = 32  # Batch size\n",
    "epochs = 5  # Number of training epochs\n",
    "es_patience = 200  # Patience fot early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\Documents\\Rodrigo\\Insper\\SextoSemestre\\Facial-Emotion-Classification-Graph_Fork\\angry_adj\n",
      "8024\n"
     ]
    }
   ],
   "source": [
    "# Load one adjacency matrix and its features\n",
    "# teste\n",
    "# load 10 npz files from the folder angry_adj\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "actual_path = pathlib.Path().absolute()\n",
    "\n",
    "# subindo um nível\n",
    "\n",
    "path = actual_path.parent\n",
    "\n",
    "\n",
    "# load the npz file\n",
    "# the npz file is a sparse matrix\n",
    "\n",
    "adj_angry_path = path / 'angry_adj'\n",
    "print(adj_angry_path)\n",
    "for file in os.listdir(adj_angry_path)[:1]:\n",
    "    file_path = adj_angry_path / file\n",
    "    sparse_matrix = scipy.sparse.load_npz(file_path)\n",
    "    sparse_matrix = sparse_matrix.todense()\n",
    "\n",
    "# as all the matrices are equal, we can use the first one is equal to all the others\n",
    "\n",
    "angry_path = path / 'surprised_dist'\n",
    "disgusted_path = path / 'disgusted_dist'\n",
    "happy_path = path / 'happy_dist'\n",
    "neutral_path = path / 'neutral_dist'\n",
    "sad_path = path / 'sad_dist'\n",
    "surprised_path = path / 'surprised_dist'\n",
    "\n",
    "path_list = [angry_path, disgusted_path, happy_path, neutral_path, sad_path, surprised_path]\n",
    "\n",
    "# load the json files of the distances (features)\n",
    "\n",
    "import json\n",
    "\n",
    "# load the json files\n",
    "dists_surprised = []\n",
    "dists_disgusted = []\n",
    "dists_happy = []\n",
    "dists_neutral = []\n",
    "dists_sad = []\n",
    "dists_angry = []\n",
    "\n",
    "dists_list = [dists_angry, dists_disgusted, dists_happy, dists_neutral, dists_sad, dists_surprised]\n",
    "\n",
    "for path in path_list:\n",
    "    for file in os.listdir(path):\n",
    "        file_path = path / file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dists_list[path_list.index(path)].append(data)\n",
    "\n",
    "print(len(dists_angry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 468)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n"
     ]
    }
   ],
   "source": [
    "# dimension of the target\n",
    "n_out = 6\n",
    "\n",
    "N = (sparse_matrix.shape[0])  # Number of nodes in the graph\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the features\n",
    "F = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "y = np.arange(n_out)  # Target labels\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# extract all nodes from the adjacency matrix\n",
    "\n",
    "def extract_nodes(adj_matrix):\n",
    "    nodes = []\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        nodes.append(i)\n",
    "    return nodes\n",
    "\n",
    "nodes = extract_nodes(sparse_matrix)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 468)\n",
      "[[ 0.         89.63977946  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [41.13040241  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# create a mtrix that is the adjancency matrix multiplied by the features\n",
    "# the features are the distances between the nodes\n",
    "# each line of the matrix is a node and the columns are the distances between the node and the other nodes\n",
    "# para cada ponto = 1 na matriz de adjacencia, multiplica pela distancia entre os pontos\n",
    "# o resultado é a matriz de features\n",
    "\n",
    "def create_features_matrix(adj_matrix, dists_list):\n",
    "    features_matrix = np.zeros((adj_matrix.shape[0], adj_matrix.shape[0]))\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                #print(dists_list[i])\n",
    "                features_matrix[i, j] = dists_list[i]\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "features_matrix = create_features_matrix(sparse_matrix, dists_list[0][0])\n",
    "print(features_matrix.shape)\n",
    "print(features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rodri\\Documents\\Rodrigo\\Insper\\SextoSemestre\\Facial-Emotion-Classification-Graph_Fork\\GNN\\model_test_gnn.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Documents/Rodrigo/Insper/SextoSemestre/Facial-Emotion-Classification-Graph_Fork/GNN/model_test_gnn.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_matrix_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Documents/Rodrigo/Insper/SextoSemestre/Facial-Emotion-Classification-Graph_Fork/GNN/model_test_gnn.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m dists \u001b[39min\u001b[39;00m dists_list:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Documents/Rodrigo/Insper/SextoSemestre/Facial-Emotion-Classification-Graph_Fork/GNN/model_test_gnn.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     features_matrix \u001b[39m=\u001b[39m create_features_matrix(sparse_matrix, dists[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Documents/Rodrigo/Insper/SextoSemestre/Facial-Emotion-Classification-Graph_Fork/GNN/model_test_gnn.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     features_matrix_list\u001b[39m.\u001b[39mappend(features_matrix)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Documents/Rodrigo/Insper/SextoSemestre/Facial-Emotion-Classification-Graph_Fork/GNN/model_test_gnn.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(features_matrix_list))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# for each distance matrix, create a features matrix\n",
    "features_matrix_list = []\n",
    "for dists in dists_list:\n",
    "    print(dists)\n",
    "    features_matrix = create_features_matrix(sparse_matrix, dists[0])\n",
    "    features_matrix_list.append(features_matrix)\n",
    "\n",
    "print(len(features_matrix_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://graphneural.network/layers/convolution/#gcsconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\envs\\mirror\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 468, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(468, 468)]                 0         []                            \n",
      "                                                                                                  \n",
      " gcs_conv (GCSConv)          (None, 468, 32)              96        ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " gcs_conv_1 (GCSConv)        (None, 468, 32)              2080      ['gcs_conv[0][0]',            \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 14976)                0         ['gcs_conv_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 512)                  7668224   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 6)                    3078      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7673478 (29.27 MB)\n",
      "Trainable params: 7673478 (29.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "fltr = normalized_adjacency(sparse_matrix)\n",
    "\n",
    "# Model using GCSConv\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "graph_conv_1 = GCSConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([X_in, A_in])\n",
    "graph_conv_2 = GCSConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([graph_conv_1, A_in])\n",
    "flatten = Flatten()(graph_conv_2)\n",
    "fc = Dense(512, activation=\"relu\")(flatten)\n",
    "output = Dense(n_out, activation=\"softmax\")(fc)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GSCConv model\n",
    "model.fit(\n",
    "    [nodes, fltr],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://graphneural.network/layers/convolution/#gcnconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "fltr = gcn_filter(adj)\n",
    "# Model using GCNConv\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "graph_conv_1 = GCNConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([X_in, A_in])\n",
    "graph_conv_2 = GCNConv(\n",
    "    32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True\n",
    ")([graph_conv_1, A_in])\n",
    "# removed the flatten layer because it is not necessary for GCNConv\n",
    "fc = Dense(512, activation=\"relu\")(graph_conv_2)\n",
    "output = Dense(n_out, activation=\"softmax\")(fc)\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GCNConv model\n",
    "\n",
    "model.fit(\n",
    "    [nodes, fltr],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirror",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
